{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9526206,"sourceType":"datasetVersion","datasetId":5800890}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-10-06T12:23:29.695490Z","iopub.execute_input":"2024-10-06T12:23:29.695941Z","iopub.status.idle":"2024-10-06T12:23:29.701287Z","shell.execute_reply.started":"2024-10-06T12:23:29.695891Z","shell.execute_reply":"2024-10-06T12:23:29.700386Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nimport copy\nimport glob\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport matplotlib.pyplot as plt","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2024-10-06T12:23:29.703200Z","iopub.execute_input":"2024-10-06T12:23:29.703480Z","iopub.status.idle":"2024-10-06T12:23:29.716705Z","shell.execute_reply.started":"2024-10-06T12:23:29.703449Z","shell.execute_reply":"2024-10-06T12:23:29.715966Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_path=\"/kaggle/input/cropped/cropped\"\n\ntrain_transform = transforms.Compose(\n    [\n        transforms.Resize([265,256]),\n        #transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Safer scale for cropping\n        transforms.RandomHorizontalFlip(),  # Safe and common for pre-trained models\n        transforms.RandomRotation(degrees=10),  # Small rotation for robustness\n        transforms.RandomAffine(degrees=0, shear=10),\n        transforms.ToTensor(),\n    ]\n)\n\n# Transformations for the validation dataset\nval_transform = transforms.Compose(\n    [\n        transforms.Resize([256, 256]),\n        #transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        \n    ]\n)\n\n# Load the entire dataset\nfull_dataset = torchvision.datasets.ImageFolder(\n    root=data_path,\n    transform=None  # No transform applied initially\n)\ntrain_size = int(0.80 * len(full_dataset))  # 80% for training\nval_size = len(full_dataset) - train_size  # 20% for validation\ntorch.manual_seed(42)\ntrain_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\ntrain_dataset.dataset.transform = train_transform\nval_dataset.dataset.transform = val_transform\nimport json\n\n# Save the indices\ntrain_indices = train_dataset.indices\nval_indices = val_dataset.indices\n\nwith open('dataset_split.json', 'w') as f:\n    json.dump({'train': train_indices, 'val': val_indices}, f)\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size=32,\n    num_workers=4,\n    shuffle=True\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-06T12:23:29.717813Z","iopub.execute_input":"2024-10-06T12:23:29.718103Z","iopub.status.idle":"2024-10-06T12:23:29.756405Z","shell.execute_reply.started":"2024-10-06T12:23:29.718072Z","shell.execute_reply":"2024-10-06T12:23:29.755411Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch = next(iter(train_loader))\nprint(batch[0].shape)\nplt.imshow(batch[0][0].permute(1, 2, 0))\nprint(batch[1][0])\n","metadata":{"execution":{"iopub.status.busy":"2024-10-06T12:23:29.759321Z","iopub.execute_input":"2024-10-06T12:23:29.759752Z","iopub.status.idle":"2024-10-06T12:23:30.682059Z","shell.execute_reply.started":"2024-10-06T12:23:29.759719Z","shell.execute_reply":"2024-10-06T12:23:30.681017Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"resnet50 = models.resnet50(weights=\"DEFAULT\")","metadata":{"execution":{"iopub.status.busy":"2024-10-06T12:23:30.683525Z","iopub.execute_input":"2024-10-06T12:23:30.683949Z","iopub.status.idle":"2024-10-06T12:23:31.171820Z","shell.execute_reply.started":"2024-10-06T12:23:30.683901Z","shell.execute_reply":"2024-10-06T12:23:31.170822Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(resnet50)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T12:23:31.173268Z","iopub.execute_input":"2024-10-06T12:23:31.174212Z","iopub.status.idle":"2024-10-06T12:23:31.181487Z","shell.execute_reply.started":"2024-10-06T12:23:31.174166Z","shell.execute_reply":"2024-10-06T12:23:31.180575Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# # Feature Extracting a Pretrained Model\n\nSince this pretrained model is trained on ImageNet dataset, the output layers has 1000 nodes. We want to reshape this last classifier layer to fit this dataset which has 2 classes. Furthermore, in feature extracting, we don't need to calculate gradient for any layers except the last layer that we initialize. For this we need to set `.requires_grad` to `False`","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def set_parameter_requires_grad(model, feature_extracting=True):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False\n            \nset_parameter_requires_grad(resnet50)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T12:23:31.182853Z","iopub.execute_input":"2024-10-06T12:23:31.183211Z","iopub.status.idle":"2024-10-06T12:23:31.191362Z","shell.execute_reply.started":"2024-10-06T12:23:31.183169Z","shell.execute_reply":"2024-10-06T12:23:31.190538Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize new output layer\nresnet50.fc = nn.Linear(2048, 5)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T12:23:31.192519Z","iopub.execute_input":"2024-10-06T12:23:31.192812Z","iopub.status.idle":"2024-10-06T12:23:31.200594Z","shell.execute_reply.started":"2024-10-06T12:23:31.192781Z","shell.execute_reply":"2024-10-06T12:23:31.199596Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check which layer in the model that will compute the gradient\nfor name, param in resnet50.named_parameters():\n    if param.requires_grad:\n        print(name, param.data)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T12:23:31.203905Z","iopub.execute_input":"2024-10-06T12:23:31.204194Z","iopub.status.idle":"2024-10-06T12:23:31.212363Z","shell.execute_reply.started":"2024-10-06T12:23:31.204164Z","shell.execute_reply":"2024-10-06T12:23:31.211402Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, dataloaders, criterion, optimizer, device, num_epochs=50, is_train=True):\n    since = time.time()\n    \n    acc_history = []\n    loss_history = []\n\n    best_acc = 0.0\n    \n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        running_loss = 0.0\n        running_corrects = 0\n\n        # Iterate over data.\n        for inputs, labels in dataloaders:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            model.to(device)\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # forward\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            _, preds = torch.max(outputs, 1)\n\n            # backward\n            loss.backward()\n            optimizer.step()\n\n            # statistics\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n\n        epoch_loss = running_loss / len(dataloaders.dataset)\n        epoch_acc = running_corrects.double() / len(dataloaders.dataset)\n\n        print('Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n\n        if epoch_acc > best_acc:\n            best_acc = epoch_acc\n\n        acc_history.append(epoch_acc.item())\n        loss_history.append(epoch_loss)\n        \n        torch.save(model.state_dict(), os.path.join('/kaggle/working/', '{0:0=2d}.pth'.format(epoch)))\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n    print('Best Acc: {:4f}'.format(best_acc))\n    \n    return acc_history, loss_history","metadata":{"execution":{"iopub.status.busy":"2024-10-06T12:23:31.213615Z","iopub.execute_input":"2024-10-06T12:23:31.213917Z","iopub.status.idle":"2024-10-06T12:23:31.226014Z","shell.execute_reply.started":"2024-10-06T12:23:31.213885Z","shell.execute_reply":"2024-10-06T12:23:31.225077Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Here we only want to update the gradient for the classifier layer that we initialized.\nparams_to_update = []\nfor name,param in resnet50.named_parameters():\n    if param.requires_grad == True:\n        params_to_update.append(param)\n        print(\"\\t\",name)\n            \noptimizer = optim.Adam(params_to_update)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T12:23:31.227205Z","iopub.execute_input":"2024-10-06T12:23:31.227493Z","iopub.status.idle":"2024-10-06T12:23:31.237899Z","shell.execute_reply.started":"2024-10-06T12:23:31.227463Z","shell.execute_reply":"2024-10-06T12:23:31.236974Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nimport torch\nimport torch.nn as nn\n\nimport torch\nimport torch.nn as nn\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n\n    def forward(self, inputs, targets):\n        # Compute softmax probabilities (BCE Loss)\n        BCE_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n        \n        # Calculate probabilities\n        pt = torch.exp(-BCE_loss)  # Probabilities for each class\n\n        # Make sure alpha is applied correctly\n        if self.alpha is not None:\n            # Ensure alpha is on the same device and of the correct shape\n            alpha_t = self.alpha[targets]  # Select alpha for the correct class\n        else:\n            alpha_t = torch.ones_like(pt)  # Default to ones if alpha is not used\n\n        # Compute Focal Loss\n        F_loss = alpha_t * (1 - pt) ** self.gamma * BCE_loss\n        \n        # Apply reduction method\n        if self.reduction == 'mean':\n            return F_loss.mean()\n        elif self.reduction == 'sum':\n            return F_loss.sum()\n        else:\n            return F_loss\nimport torch\nimport torch.nn as nn\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n\n    def forward(self, inputs, targets):\n        # Compute softmax probabilities (BCE Loss)\n        BCE_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n        \n        # Calculate probabilities\n        pt = torch.exp(-BCE_loss)  # Probabilities for each class\n\n        # Make sure alpha is applied correctly\n        if self.alpha is not None:\n            # Ensure alpha is on the same device and of the correct shape\n            alpha_t = self.alpha[targets]  # Select alpha for the correct class\n        else:\n            alpha_t = torch.ones_like(pt)  # Default to ones if alpha is not used\n\n        # Compute Focal Loss\n        F_loss = alpha_t * (1 - pt) ** self.gamma * BCE_loss\n        \n        # Apply reduction method\n        if self.reduction == 'mean':\n            return F_loss.mean()\n        elif self.reduction == 'sum':\n            return F_loss.sum()\n        else:\n            return F_loss\n\n\nweights=[8.4147,12,5,2.7538,15]\nclass_weights = torch.tensor(weights).to(device)\ncriterion = FocalLoss(alpha=class_weights).to(device)\n\n# Train model\ntrain_acc_hist, train_loss_hist = train_model(resnet50, train_loader, criterion, optimizer, device)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T12:23:31.239309Z","iopub.execute_input":"2024-10-06T12:23:31.239634Z","iopub.status.idle":"2024-10-06T12:29:38.122078Z","shell.execute_reply.started":"2024-10-06T12:23:31.239593Z","shell.execute_reply":"2024-10-06T12:29:38.120923Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#test_path = r\"C:\\Users\\HP\\Desktop\\IISC\\Sem 1\\Machine Learning for Cyber Physical Systems\\Project 1\\ML4CPS-Project1-\\Test_Data\"\n\ntest_loader = torch.utils.data.DataLoader(\n    val_dataset,\n    batch_size=32,\n    num_workers=1,\n    shuffle=False\n)\n\nprint(len(test_loader))","metadata":{"execution":{"iopub.status.busy":"2024-10-06T12:29:38.124023Z","iopub.execute_input":"2024-10-06T12:29:38.124963Z","iopub.status.idle":"2024-10-06T12:29:38.130858Z","shell.execute_reply.started":"2024-10-06T12:29:38.124906Z","shell.execute_reply":"2024-10-06T12:29:38.129942Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def eval_model(model, dataloaders, device):\n    since = time.time()\n    \n    acc_history = []\n    best_acc = 0.0\n\n    saved_models = glob.glob('/kaggle/working/' + '*.pth')\n    saved_models.sort()\n    print('saved_model', saved_models)\n\n    for model_path in saved_models:\n        print('Loading model', model_path)\n\n        model.load_state_dict(torch.load(model_path))\n        model.eval()\n        model.to(device)\n\n        running_corrects = 0\n        # Iterate over data.\n        for inputs, labels in dataloaders:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            with torch.no_grad():\n                outputs = model(inputs)\n\n            _, preds = torch.max(outputs, 1)\n            running_corrects += torch.sum(preds == labels.data)\n\n        epoch_acc = running_corrects.double() / len(dataloaders.dataset)\n\n        print('Acc: {:.4f}'.format(epoch_acc))\n        \n        if epoch_acc > best_acc:\n            best_acc = epoch_acc\n\n        acc_history.append(epoch_acc.item())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Validation complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n    print('Best Acc: {:4f}'.format(best_acc))\n    \n    return acc_history","metadata":{"execution":{"iopub.status.busy":"2024-10-06T12:29:38.132073Z","iopub.execute_input":"2024-10-06T12:29:38.132424Z","iopub.status.idle":"2024-10-06T12:29:38.149153Z","shell.execute_reply.started":"2024-10-06T12:29:38.132381Z","shell.execute_reply":"2024-10-06T12:29:38.148324Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_acc_hist = eval_model(resnet50, test_loader, device)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T12:29:38.150588Z","iopub.execute_input":"2024-10-06T12:29:38.151447Z","iopub.status.idle":"2024-10-06T12:31:33.475414Z","shell.execute_reply.started":"2024-10-06T12:29:38.151400Z","shell.execute_reply":"2024-10-06T12:31:33.474278Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(train_acc_hist)\nplt.plot(val_acc_hist)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-06T12:31:33.476925Z","iopub.execute_input":"2024-10-06T12:31:33.477277Z","iopub.status.idle":"2024-10-06T12:31:33.721858Z","shell.execute_reply.started":"2024-10-06T12:31:33.477237Z","shell.execute_reply":"2024-10-06T12:31:33.721062Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Hi\")","metadata":{"execution":{"iopub.status.busy":"2024-10-06T12:31:33.722997Z","iopub.execute_input":"2024-10-06T12:31:33.723275Z","iopub.status.idle":"2024-10-06T12:31:33.728152Z","shell.execute_reply.started":"2024-10-06T12:31:33.723245Z","shell.execute_reply":"2024-10-06T12:31:33.727258Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(val_loss_hist)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-06T12:31:33.729436Z","iopub.execute_input":"2024-10-06T12:31:33.729821Z","iopub.status.idle":"2024-10-06T12:31:33.785921Z","shell.execute_reply.started":"2024-10-06T12:31:33.729778Z","shell.execute_reply":"2024-10-06T12:31:33.784752Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\n\n# Define the path to your test images\ntest_data_path = \"/kaggle/input/faltu-dataset/Project 1 Data/Test_Data\"\n\n\n# Custom dataset class for loading images from a single folder\nclass CustomImageDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.image_files = [f for f in os.listdir(root_dir) if f.endswith(('jpg', 'jpeg', 'png'))]\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.root_dir, self.image_files[idx])\n        image = Image.open(img_name).convert('RGB')  # Ensure image is in RGB format\n        if self.transform:\n            image = self.transform(image)\n        return image\n\n# Create an instance of the custom dataset\ntest_dataset = CustomImageDataset(root_dir=test_data_path, transform=val_transform)\n\n# Create DataLoader for the test dataset\ntest1_loader = DataLoader(\n    test_dataset,\n    batch_size=32,\n    num_workers=4,\n    shuffle=False  # No need to shuffle test data\n)\n\n# Example of making predictions\n# Load your trained model\n# model = YourModel()\n# model.load_state_dict(torch.load('your_model.pth'))\n# model.eval()  # Set the model to evaluation mode\nmodel=resnet50\n# Device configuration\n#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# model.to(device)\n\n# Make predictions\npredictions = []\nwith torch.no_grad():  # Disable gradient calculation\n    for images in test1_loader:\n        images = images.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        predictions.extend(predicted.cpu().numpy())\n\nprint(predictions)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-06T12:31:33.786859Z","iopub.status.idle":"2024-10-06T12:31:33.787218Z","shell.execute_reply.started":"2024-10-06T12:31:33.787037Z","shell.execute_reply":"2024-10-06T12:31:33.787055Z"},"trusted":true},"outputs":[],"execution_count":null}]}